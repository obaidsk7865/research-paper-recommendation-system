# -*- coding: utf-8 -*-
"""Recommend Papers to Read using similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y17Hsn5cJHgXN6mH0OP73DkV4hjX7sH0
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# This file have more datapoints than the other file
df = pd.read_csv("/content/arxiv_data.csv")
df.head()

df.shape

for col in df:
    print(f"{col} have {df[col].nunique()} unique values ")

df.drop(columns = ["terms","summaries"], inplace = True)

df.drop_duplicates(inplace= True)
df.reset_index(drop= True,inplace = True)
df

pd.set_option('display.max_colwidth', None)
df.head()

!pip install -U -q sentence-transformers

from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-MiniLM-L6-v2')

#Our sentences we like to encode
sentences = df['titles']

#Sentences are encoded by calling model.encode()
embeddings = model.encode(sentences)

#Print the embeddings
c = 0
for sentence, embedding in zip(sentences, embeddings):
    print("Sentence:", sentence)
    print("Embedding length:", len(embedding)) # list of floats
    print("")
    if c >=5:
        break
    c +=1

import pickle
# Saving sentences and corresponding embeddings
with open('embeddings.pkl', 'wb') as f:
    pickle.dump(embeddings, f)

with open('sentences.pkl', 'wb') as f:
    pickle.dump(sentences, f)

paper_you_like = input("Enter the title of any paper you like")

paper_you_like

from sentence_transformers import util
cosine_scores = util.cos_sim(embeddings, model.encode(paper_you_like))

import torch
top_similar_papers = torch.topk(cosine_scores,dim=0, k=5,sorted=True)
top_similar_papers

cosine_scores.shape

for i in top_similar_papers.indices:
#     print(i)
    print(sentences[i.item()])

user_paper_list = []
add_more = 'y'
while add_more =='y' or add_more =='Y':
    add_more = 'n'
    paper_you_like = input("Enter the title of any paper you like")
    user_paper_list.append(paper_you_like)
    add_more = input("Enter y/Y to add more papers and n/N to exit")

user_paper_list

from sentence_transformers import util
cosine_scores = util.cos_sim(embeddings, model.encode(user_paper_list))

cosine_scores.shape

cosine_scores[0]

top_similar_papers = torch.topk(cosine_scores,dim=0, k=10,sorted=True)
# top_similar_papers = torch.topk(cosine_scores.flatten(), k=10,sorted=True)
top_similar_papers

sentence_index =  top_similar_papers.indices.flatten()
sentence_index, sentence_index.shape

topmost_match = torch.topk(top_similar_papers.values.flatten(),dim=0,k=10)
topmost_match # These match have highest similarity with all the input titles

for i in topmost_match.indices:
    print(sentences[ sentence_index[i].item() ])

